{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "375425e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import env\n",
    "import numpy as np\n",
    "import acquire\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3989e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from local CSV...\n"
     ]
    }
   ],
   "source": [
    "df = acquire.zillow_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['calculatedbathnbr',\n",
    "                'finishedfloor1squarefeet',\n",
    "                'finishedsquarefeet12', \n",
    "                'regionidcity',\n",
    "                'landtaxvaluedollarcnt',\n",
    "                'taxamount',\n",
    "                'rawcensustractandblock']\n",
    "\n",
    "cols_to_fill_zero = ['fireplacecnt',\n",
    "                     'garagecarcnt',\n",
    "                     'garagetotalsqft',\n",
    "                     'hashottuborspa',\n",
    "                     'poolcnt',\n",
    "                     'threequarterbathnbr',\n",
    "                     'taxdelinquencyflag']\n",
    "\n",
    "def prep_zillow(df, cols_to_drop, cols_to_fill_zero):\n",
    "    # drop redundant id code columns\n",
    "    id_cols = [col for col in df.columns if 'typeid' in col or col in ['id', 'parcelid']]\n",
    "    df = df.drop(columns=id_cols)\n",
    "    # filter for single family properties\n",
    "    df = df[df.propertylandusedesc == 'Single Family Residential']\n",
    "    # drop specified columns\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    # fill null values with 0 in specified columns\n",
    "    for col in cols_to_fill_zero:\n",
    "        df[col] = np.where(df[col].isna(), 0, df[col])    \n",
    "        \n",
    "        \n",
    "    # return cleaned df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_zillow_1(df):\n",
    "    '''\n",
    "    This function takes in a dataframe of zillow data obtained using the acquire.zillow_2017_data function. \n",
    "\n",
    "    It replaces null values for garage area with 0's\n",
    "\n",
    "    It checks for null values and removes all observations containing null values if the number of null values is \n",
    "    less than 5% the total number of observations. \n",
    "\n",
    "    It renames feature columns for readability and adherence to snake_case conventions. \n",
    "\n",
    "    It creates a feature, 'age', by subtracting year_built from the year of the transactions (2017), then\n",
    "    drops the original year_built column. \n",
    "\n",
    "    It changes fips codes from number types to a string type. \n",
    "\n",
    "    The cleaned dataframe is returned. \n",
    "    '''\n",
    "    # replace null values for garagetotalsqft and poolcnt with 0\n",
    "    df['garagetotalsqft'] = np.where(df.garagetotalsqft.isna(), 0, df.garagetotalsqft)\n",
    "    df['poolcnt'] = np.where(df.poolcnt.isna(), 0, df.poolcnt)\n",
    "    # renaming columns for readability\n",
    "    df = df.rename(columns = {'bedroomcnt': 'bedrooms',\n",
    "                              'bathroomcnt': 'bathrooms', \n",
    "                              'calculatedfinishedsquarefeet': 'sqft', \n",
    "                              'taxvaluedollarcnt': 'tax_value',\n",
    "                              'yearbuilt': 'year_built',\n",
    "                              'garagetotalsqft': 'garage_sqft',\n",
    "                              'poolcnt': 'pools',\n",
    "                              'lotsizesquarefeet': 'lot_sqft'})\n",
    "    # check for null values\n",
    "    total_nulls = df.isnull().sum().sum()\n",
    "    # if the total number of nulls is less than 5% of the number of observations in the df\n",
    "    if total_nulls / len(df) < .05:\n",
    "        # drop all rows containing null values\n",
    "        df = df.dropna()\n",
    "    else:\n",
    "        print('Number of null values > 5% length of df. Evaluate further before dropping nulls.')\n",
    "        return None \n",
    "\n",
    "    # changing data types:\n",
    "    # changing year from float to int\n",
    "    df['year_built'] = df.year_built.apply(lambda year: int(year))\n",
    "    # adding a feature: age \n",
    "    df['age'] = 2017 - df.year_built\n",
    "    # drop original year_built_column\n",
    "    df = df.drop(columns='year_built')\n",
    "    # changing fips codes to strings\n",
    "    df['fips'] = df.fips.apply(lambda fips: '0' + str(int(fips)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3694d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_test_validate_split(df, test_size=.2, validate_size=.3, random_state=42):\n",
    "    '''\n",
    "    This function takes in a dataframe, then splits that dataframe into three separate samples\n",
    "    called train, test, and validate, for use in machine learning modeling.\n",
    "\n",
    "    Three dataframes are returned in the following order: train, test, validate. \n",
    "    \n",
    "    The function also prints the size of each sample.\n",
    "    '''\n",
    "    # split the dataframe into train and test\n",
    "    train, test = train_test_split(df, test_size=.2, random_state=42)\n",
    "    # further split the train dataframe into train and validate\n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=42)\n",
    "    # print the sample size of each resulting dataframe\n",
    "    print(f'train\\t n = {train.shape[0]}')\n",
    "    print(f'test\\t n = {test.shape[0]}')\n",
    "    print(f'validate n = {validate.shape[0]}')\n",
    "\n",
    "    return train, test, validate\n",
    "\n",
    "# def remove_outliers(train, validate, test, k, col_list):\n",
    "#     ''' \n",
    "#     This function takes in a dataset split into three sample dataframes: train, validate and test.\n",
    "#     It calculates an outlier range based on a given value for k, using the interquartile range \n",
    "#     from the train sample. It then applies that outlier range to each of the three samples, removing\n",
    "#     outliers from a given list of feature columns. The train, validate, and test dataframes \n",
    "#     are returned, in that order. \n",
    "#     '''\n",
    "#     # iterate through each column in the given list\n",
    "#     for col in col_list:\n",
    "#         q1, q3 = train[col].quantile([.25, .75])  # establish the 1st and 3rd quartiles\n",
    "#         iqr = q3 - q1   # calculate interquartile range\n",
    "#         upper_bound = q3 + k * iqr   # get upper bound\n",
    "#         lower_bound = q1 - k * iqr   # get lower bound\n",
    "#         # remove outliers from each of the three samples\n",
    "#         train = train[(train[col] > lower_bound) & (train[col] < upper_bound)]\n",
    "#         validate = validate[(validate[col] > lower_bound) & (validate[col] < upper_bound)]\n",
    "#         test = test[(test[col] > lower_bound) & (test[col] < upper_bound)]\n",
    "#     # print the sample size of each resulting dataframe\n",
    "#     print(f'train\\t n = {train.shape[0]}')\n",
    "#     print(f'test\\t n = {test.shape[0]}')\n",
    "#     print(f'validate n = {validate.shape[0]}')\n",
    "#     #return sample dataframes without outliers\n",
    "#     return train, validate, test\n",
    "\n",
    "def remove_outliers(train, validate, test, k, col_list):\n",
    "    ''' \n",
    "    This function takes in a dataset split into three sample dataframes: train, validate and test.\n",
    "    It calculates an outlier range based on a given value for k, using the interquartile range \n",
    "    from the train sample. It then applies that outlier range to each of the three samples, removing\n",
    "    outliers from a given list of feature columns. The train, validate, and test dataframes \n",
    "    are returned, in that order. \n",
    "    '''\n",
    "    # Create a column that will label our rows as containing an outlier value or not\n",
    "    train['outlier'] = False\n",
    "    validate['outlier'] = False\n",
    "    test['outlier'] = False\n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = train[col].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # update the outlier label any time that the value is outside of boundaries\n",
    "        train['outlier'] = np.where(((train[col] < lower_bound) | (train[col] > upper_bound)) & (train.outlier == False), True, train.outlier)\n",
    "        validate['outlier'] = np.where(((validate[col] < lower_bound) | (validate[col] > upper_bound)) & (validate.outlier == False), True, validate.outlier)\n",
    "        test['outlier'] = np.where(((test[col] < lower_bound) | (test[col] > upper_bound)) & (test.outlier == False), True, test.outlier)\n",
    "\n",
    "    # remove observations with the outlier label in each of the three samples\n",
    "    train = train[train.outlier == False]\n",
    "    train = train.drop(columns=['outlier'])\n",
    "\n",
    "    validate = validate[validate.outlier == False]\n",
    "    validate = validate.drop(columns=['outlier'])\n",
    "\n",
    "    test = test[test.outlier == False]\n",
    "    test = test.drop(columns=['outlier'])\n",
    "\n",
    "    # print the remaining \n",
    "    print(f'train\\t n = {train.shape[0]}')\n",
    "    print(f'test\\t n = {test.shape[0]}')\n",
    "    print(f'validate n = {validate.shape[0]}')\n",
    "\n",
    "    return train, validate, test\n",
    "\n",
    "def scale_zillow(train, validate, test, target, scaler_type=MinMaxScaler()):\n",
    "    '''\n",
    "    This takes in the train, validate, and test dataframes, as well as the target label. \n",
    "\n",
    "    It then fits a scaler object to the train sample based on the given sample_type, applies that\n",
    "    scaler to the train, validate, and test samples, and appends the new scaled data to the \n",
    "    dataframes as additional columns with the prefix 'scaled_'. \n",
    "\n",
    "    train, validate, and test dataframes are returned, in that order. \n",
    "    '''\n",
    "    # identify quantitative features to scale\n",
    "    quant_features = [col for col in train.columns if (train[col].dtype != 'object') & (col != target)]\n",
    "    # establish empty dataframes for storing scaled dataset\n",
    "    train_scaled = pd.DataFrame(index=train.index)\n",
    "    validate_scaled = pd.DataFrame(index=validate.index)\n",
    "    test_scaled = pd.DataFrame(index=test.index)\n",
    "    # screate and fit the scaler\n",
    "    scaler = scaler_type.fit(train[quant_features])\n",
    "    # adding scaled features to scaled dataframes\n",
    "    train_scaled[quant_features] = scaler.transform(train[quant_features])\n",
    "    validate_scaled[quant_features] = scaler.transform(validate[quant_features])\n",
    "    test_scaled[quant_features] = scaler.transform(test[quant_features])\n",
    "    # add 'scaled' prefix to columns\n",
    "    for feature in quant_features:\n",
    "        train_scaled = train_scaled.rename(columns={feature: f'scaled_{feature}'})\n",
    "        validate_scaled = validate_scaled.rename(columns={feature: f'scaled_{feature}'})\n",
    "        test_scaled = test_scaled.rename(columns={feature: f'scaled_{feature}'})\n",
    "    # concat scaled feature columns to original train, validate, test df's\n",
    "    train = pd.concat([train, train_scaled], axis=1)\n",
    "    validate = pd.concat([validate, validate_scaled], axis=1)\n",
    "    test = pd.concat([test, test_scaled], axis=1)\n",
    "\n",
    "    return train, validate, test\n",
    "\n",
    "def encode_zillow(train, validate, test, target):\n",
    "    '''\n",
    "    This function takes in the train, validate, and test samples, as well as a label for the target variable. \n",
    "\n",
    "    It then encodes each of the categorical variables using one-hot encoding with dummy variables and appends \n",
    "    the new encoded variables to the original dataframes as new columns with the prefix 'enc_{variable_name}'.\n",
    "\n",
    "    train, validate and test dataframes are returned (in that order)\n",
    "    '''\n",
    "    # identify the features to encode (categorical features represented by non-numeric data types)\n",
    "    features_to_encode = [col for col in train.columns if (train[col].dtype == 'object') & (col != target)]\n",
    "    #iterate through the list of features                  \n",
    "    for feature in features_to_encode:\n",
    "        # establish dummy variables\n",
    "        dummy_df = pd.get_dummies(train[feature],\n",
    "                                  prefix=f'enc_{train[feature].name}',\n",
    "                                  drop_first=True)\n",
    "        # add the dummies as new columns to the original dataframe\n",
    "        train = pd.concat([train, dummy_df], axis=1)\n",
    "\n",
    "    # then repeat the process for the other two samples:\n",
    "\n",
    "    for feature in features_to_encode:\n",
    "        dummy_df = pd.get_dummies(validate[feature],\n",
    "                                  prefix=f'enc_{validate[feature].name}',\n",
    "                                  drop_first=True)\n",
    "        validate = pd.concat([validate, dummy_df], axis=1)\n",
    "        \n",
    "    for feature in features_to_encode:\n",
    "        dummy_df = pd.get_dummies(test[feature],\n",
    "                                  prefix=f'enc_{test[feature].name}',\n",
    "                                  drop_first=True)\n",
    "        test = pd.concat([test, dummy_df], axis=1)\n",
    "    \n",
    "    return train, validate, test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
